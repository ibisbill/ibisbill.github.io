<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>5.4 分布式训练并行（DP/TP/PP/ZeRO/FSDP）｜Transformer × LLM 学习站</title>
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, theme: 'dark' });
  </script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 5.4</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>5.4 分布式训练并行</h1>
  <p class="muted">探讨如何将超大规模模型分布到成百上千块 GPU 上并行训练的核心策略。</p>
  <div class="pillrow">
    <span class="pill">模块 5</span>
    <span class="pill"><a href="./module-05-预训练工程与技巧.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 数据并行 (Data Parallelism, DP)</h3>
  <p>最基础的并行方式。每个 GPU 保存一份完整的模型副本，但处理不同的数据 Batch。</p>
  <ul>
    <li><strong>操作:</strong> 每块卡独立计算梯度。</li>
    <li><strong>通信:</strong> 训练每步结束后，通过 <code>All-Reduce</code> 操作将各卡的梯度求平均并同步到所有卡。</li>
    <li><strong>局限:</strong> 当模型大到单块 GPU 显存装不下时，DP 无法单独工作。</li>
  </ul>

  <h3>2. 张量并行 (Tensor Parallelism, TP)</h3>
  <p>将模型中庞大的矩阵乘法（如 $Y = XW$）切分到多块 GPU 上。通常用于 Megatron-LM。</p>
  <ul>
    <li><strong>按行/按列切分:</strong> 比如将权重 $W$ 切分为两半。</li>
    <li><strong>通信:</strong> 每次矩阵乘法后都需要进行 <code>All-Gather</code> 或 <code>All-Reduce</code> 同步中间结果。通信开销较大，通常仅在节点内的高速 NVLink 上使用。</li>
  </ul>

  <h3>3. 流水并行 (Pipeline Parallelism, PP)</h3>
  <p>将模型的不同层分配到不同的 GPU 上。GPU 0 处理前 10 层，GPU 1 处理后 10 层。</p>
  <ul>
    <li><strong>Pipeline Bubble:</strong> 后面的 GPU 必须等待前面 GPU 的计算结果，产生空闲时间（Bubble）。</li>
    <li><strong>优化:</strong> 使用 1F1B 调度和 Micro-batching 来减小空闲比例。</li>
  </ul>

  <h3>4. ZeRO (Zero Redundancy Optimizer) / FSDP</h3>
  <p>由 DeepSpeed 提出的显存优化技术，其核心思想是<strong>切分而非复制</strong>。</p>
  <ul>
    <li><strong>ZeRO-1:</strong> 切分优化器状态 (Optimizer States)。</li>
    <li><strong>ZeRO-2:</strong> 进一步切分梯度 (Gradients)。</li>
    <li><strong>ZeRO-3 (FSDP):</strong> 切分模型参数 (Parameters)。在计算时，通过 <code>All-Gather</code> 动态获取所需参数。</li>
  </ul>

  <h3>并行策略对比图</h3>
  <div style="text-align: center; margin: 2rem 0;">
    <div class="mermaid">
graph TD
    subgraph Parallelism
    DP[数据并行: 分数据]
    TP[张量并行: 分矩阵]
    PP[流水并行: 分层]
    ZeRO[ZeRO/FSDP: 分状态]
    end
    DP --> Simple[简单但受显存限制]
    TP --> IntraNode[节点内高速通信]
    PP --> InterNode[跨节点/长序列]
    ZeRO --> MemoryEfficient[显存利用率最高]
    </div>
  </div>
</div>

<div class="section">
  <h2>工程要点总结</h2>
  <ul>
    <li><strong>3D 并行:</strong> 现代万亿参数模型训练通常同时结合 TP + PP + DP (或 ZeRO)，称为 3D 并行。</li>
    <li><strong>通信重叠 (Communication Overlap):</strong> 尽量在计算下一层梯度的同时，同步上一层的梯度，以隐藏通信开销。</li>
    <li><strong>有效 Batch Size:</strong> $Total\_Batch = DP\_size \times micro\_batch \times grad\_accum$。</li>
  </ul>

  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：为什么 DP 需要 all-reduce？</div>
    <div class="a">A1：因为数据并行中，每块 GPU 看到的训练样本是不一样的，它们计算出的梯度方向也不同。为了保证训练效果等价于单机大 Batch 训练，必须将所有卡的梯度汇总求平均（All-Reduce），使得所有卡上的参数更新步调一致。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：PP 的 bubble 是什么？如何缓解？</div>
    <div class="a">A2：Bubble 是指在流水线开始和结束阶段，部分 GPU 由于正在等待前序/后续阶段的计算结果而处于闲置状态。缓解方案包括：将大 Batch 切分为多个 Micro-batch（小批次），并使用 1F1B（一个前向紧接一个反向）调度算法，让流水线被充分填充，从而减小空闲时间的比例。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-5-3-混合精度与数值稳定.html"><strong>5.3 混合精度与数值稳定</strong></a>
      <a href="kp-5-5-内存与算子优化-checkpoint-flashattention-fused.html"><strong>5.5 内存与算子优化（checkpoint/FlashAttention/fused）</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>