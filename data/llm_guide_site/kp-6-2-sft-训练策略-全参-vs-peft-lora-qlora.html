<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>6.2 SFT 训练策略：全参 vs PEFT（LoRA/QLoRA）｜Transformer × LLM 学习站</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/vs2015.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 6.2</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>6.2 SFT 训练策略：全参 vs PEFT</h1>
  <p class="muted">探讨在大模型微调中，如何权衡效果、显存与计算成本，选择最适合的训练方案。</p>
  <div class="pillrow">
    <span class="pill">模块 6</span>
    <span class="pill"><a href="./module-06-指令微调-sft-与后训练-post-training.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 全参数微调 (Full Fine-tuning)</h3>
  <p>在 SFT 时更新模型的所有参数。这是最直接的方法，能最大程度地调整模型以适应新数据分布。</p>
  <ul>
    <li><strong>优点:</strong> 上限最高，能学习复杂的指令遵循逻辑。</li>
    <li><strong>缺点:</strong> 显存压力极大（需要存储参数、梯度和优化器状态），且容易产生“灾难性遗忘”。</li>
  </ul>

  <h3>2. LoRA (Low-Rank Adaptation)</h3>
  <p>LoRA 的核心思想是冻结原始权重 $W$，而是在侧边训练一个低秩的矩阵乘积 $\Delta W = A \times B$。</p>
  <p>$$h = Wx + \Delta Wx = Wx + BAx$$</p>
  <ul>
    <li>其中 $A \in \mathbb{R}^{r \times d}$, $B \in \mathbb{R}^{k \times r}$，$r \ll \min(d, k)$ 是秩（Rank）。</li>
    <li><strong>优点:</strong> 训练参数量极少（通常小于全参的 1%），显著降低显存，且训练好的旁路可以合并回主权重中，实现推理零开销。</li>
  </ul>

  <h3>3. QLoRA (Quantized LoRA)</h3>
  <p>QLoRA 进一步将基座模型量化为 4-bit（使用 NormalFloat4 格式），同时配合分页优化器（Paged Optimizers）处理显存峰值。</p>
  <ul>
    <li><strong>核心收益:</strong> 极大降低了微调的显存门槛。例如，使用单张 24GB 的 RTX 3090/4090 即可微调 70B 参数规模的模型。</li>
  </ul>

  <h3>代码示例：使用 PEFT 库配置 LoRA</h3>
  <pre><code class="language-python">from peft import LoraConfig, get_peft_model

# 配置 LoRA 参数
config = LoraConfig(
    r=8, # 秩
    lora_alpha=32, # 缩放系数
    target_modules=["q_proj", "v_proj"], # 作用在哪些层
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# 包装原始模型
model = get_peft_model(base_model, config)
model.print_trainable_parameters()
# 输出将显示可训练参数占比通常极低
</code></pre>
</div>

<div class="section">
  <h2>工程要点总结</h2>
  <ul>
    <li><strong>Rank 的选择:</strong> 并非 $r$ 越大越好。在指令微调中，$r=8$ 或 $16$ 通常已经足够。过大的 $r$ 会增加过拟合风险并降低训练稳定性。</li>
    <li><strong>可变性:</strong> 全参微调适合基础能力的深度对齐；LoRA 适合多租户模型部署（只需切换轻量的适配器文件）。</li>
    <li><strong>灾难性遗忘防护:</strong> 微调时应混入 5%-10% 的原始预训练数据作为锚定，防止模型在习得新能力的同时丧失通用常识。</li>
  </ul>

  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：LoRA 的核心思想？</div>
    <div class="a">A1：LoRA 假设权重的更新（$\Delta W$）具有很低的“内在维度”。因此，它冻结了原始的高维权重矩阵 $W$，改为训练两个极小的低秩矩阵 $A$ 和 $B$ 的乘积来近似这个更新。这极大地减少了需要更新的参数量和显存需求。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：QLoRA 的关键工程收益？</div>
    <div class="a">A2：QLoRA 通过 4-bit 量化极大压缩了模型权重的显存占用，并引入了“双量化”和“分页优化器”技术来进一步压榨显存。这使得开发者能够在消费级显卡上微调原本只有企业级服务器才能跑动的超大模型，极大地降低了 LLM 研究的门槛。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-6-1-sft-数据-质量-覆盖与模板.html"><strong>6.1 SFT 数据：质量、覆盖与模板</strong></a>
      <a href="kp-6-3-后训练方向-安全风格-工具格式与领域适配.html"><strong>6.3 后训练方向：安全风格、工具格式与领域适配</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>