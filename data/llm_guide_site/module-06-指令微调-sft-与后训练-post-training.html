<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>6 指令微调（SFT）与后训练（Post-training）｜Transformer × LLM 学习站</title>
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">模块 6</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>6 指令微调（SFT）与后训练（Post-training）</h1>
  <p class="muted">本模块涵盖了将预训练模型转化为“智能助手”的关键步骤：高质量指令数据的构建、高效微调策略以及面向业务场景的专项强化。</p>
</div>

<div class="grid">
  
<div class="card third">
  <h2><a href="./kp-6-1-sft-数据-质量-覆盖与模板.html">6.1 SFT 数据：质量、覆盖与模板</a></h2>
  <p class="muted">掌握指令数据的筛选标准与标准的对话模板设计。</p>
</div>

<div class="card third">
  <h2><a href="./kp-6-2-sft-训练策略-全参-vs-peft-lora-qlora.html">6.2 SFT 训练策略：全参 vs PEFT</a></h2>
  <p class="muted">权衡全量微调与 LoRA/QLoRA 在不同资源下的表现。</p>
</div>

<div class="card third">
  <h2><a href="./kp-6-3-后训练方向-安全风格-工具格式与领域适配.html">6.3 后训练方向：安全、工具与领域</a></h2>
  <p class="muted">针对安全性、外部工具调用及行业需求的深度定制。</p>
</div>

</div>

<div class="section">
  <h2>模块概述</h2>
  <p>如果说预训练是赋予模型“知识”，那么指令微调（SFT）就是教会模型“规矩”和“技能”。在这一阶段，数据的“质”远胜于“量”。通过精心设计的对话模板与高效的参数微调技术（如 LoRA），我们可以让模型在保留通用能力的同时，迅速适应特定任务。此外，通过后训练（Post-training）阶段的专项强化，模型将具备更强的工具使用能力和更高的安全合规水平，从而真正具备落地应用的能力。</p>
</div>

    <div class="pager">
      <a href="kp-5-7-续训-领域继续预训练与长上下文扩展.html"><strong>5.7 续训、领域继续预训练与长上下文扩展</strong></a>
      <a href="kp-6-1-sft-数据-质量-覆盖与模板.html"><strong>6.1 SFT 数据：质量、覆盖与模板</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>