<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>3.3 Seq2Seq 条件生成损失｜Transformer × LLM 学习站</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/vs2015.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, theme: 'default' });
  </script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 3.3</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>3.3 Seq2Seq 条件生成损失</h1>
  <p class="muted">研究给定输入序列 $x$ 时，如何优化目标序列 $y$ 的生成概率。</p>
  <div class="pillrow">
    <span class="pill">模块 3</span>
    <span class="pill"><a href="./module-03-语言建模目标与损失.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 条件生成目标</h3>
  <p>Seq2Seq 任务的核心是建立从输入序列 $x = (x_1, \dots, x_n)$ 到输出序列 $y = (y_1, \dots, y_m)$ 的映射。其目标是最大化条件概率 $p(y | x)$：</p>
  <p>$$p(y | x) = \prod_{t=1}^m p(y_t | y_{<t}, x)$$</p>
  <p>在 Transformer 架构中，Encoder 处理输入 $x$，Decoder 结合 Encoder 的输出（通过 Cross-Attention）自回归地生成 $y$。</p>

  <h3>2. Teacher Forcing</h3>
  <p>在训练阶段，为了加速收敛，我们通常使用 <strong>Teacher Forcing</strong> 技术：在生成第 $t$ 个 token 时，Decoder 的输入是真实的标签 $y_{<t}$，而不是模型在 $t-1$ 时刻生成的预测值。</p>
  <ul>
    <li><strong>优点:</strong> 训练更稳定，收敛更快。</li>
    <li><strong>缺点:</strong> 导致曝光偏差 (Exposure Bias)，即模型在推理时没有“老师”指导，一旦出错就会累积。</li>
  </ul>

  <h3>3. 评估指标</h3>
  <ul>
    <li><strong>BLEU (Bilingual Evaluation Understudy):</strong> 主要用于翻译，通过计算 n-gram 重合度并结合长度惩罚来评估。
      $$\text{BLEU} = \text{BP} \cdot \exp\left( \sum_{n=1}^N w_n \log p_n \right)$$
    </li>
    <li><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation):</strong> 主要用于摘要，侧重于召回率，即参考摘要中有多少内容出现在模型输出中。</li>
    <li><strong>困惑度 (PPL):</strong> 同样适用，但只能反映模型对目标文本的拟合程度，不能直接衡量生成质量（如逻辑性）。</li>
  </ul>

  <h3>4. 数据流示意图</h3>
  <div style="text-align: center;">
    <div class="mermaid">
graph LR
    X[输入序列 X] --> Enc[Encoder]
    Enc --> CA[Cross-Attention]
    Y_prev[真实前缀 Y_{<t}] --> Dec[Decoder]
    CA --> Dec
    Dec --> Logits[预测 Logits]
    Logits -- Cross Entropy -- Y_t[真实目标 Y_t]
    </div>
  </div>

  <h3>代码示例：Seq2Seq 损失计算逻辑 (伪代码)</h3>
  <pre><code class="language-python"># 假设 encoder_outputs 是 Encoder 的输出
# decoder_input_ids 是带 shift 的真实标签 (Teacher Forcing)
# labels 是真实目标 token

outputs = decoder(
    input_ids=decoder_input_ids,
    encoder_hidden_states=encoder_outputs,
    # ... 其他参数
)

logits = outputs.logits
# 只对非 padding 的部分计算交叉熵
loss = F.cross_entropy(logits.view(-1, V), labels.view(-1), ignore_index=-100)
</code></pre>
</div>

<div class="section">
  <h2>工程要点总结</h2>
  <ul>
    <li><strong>Label Smoothing:</strong> 在分类时引入少量噪声，防止模型过于自信，有助于提升泛化。</li>
    <li><strong>Beam Search vs Sampling:</strong> 在推理阶段，Seq2Seq 任务通常使用 Beam Search 来寻找最高概率的序列，而开放式对话则倾向于使用 Sampling 来增加多样性。</li>
    <li><strong>Cross-Attention Masking:</strong> 确保 Decoder 在 Cross-Attention 时能看到完整的 Encoder 输出，但在 Self-Attention 时受 Causal Mask 限制。</li>
  </ul>

  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：为什么 Seq2Seq 常用 cross-attention？</div>
    <div class="a">A1：解码器需要在生成每个 token 时参考输入的上下文信息。Cross-attention 允许解码器的 Query 去查询 Encoder 输出的 Key 和 Value，从而实现这种条件依赖。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-3-2-masked-lm-denoising-目标.html"><strong>3.2 Masked LM / Denoising 目标</strong></a>
      <a href="kp-3-4-稳定性与正则化技巧-z-loss-label-smoothing-等.html"><strong>3.4 稳定性与正则化技巧（z-loss/label smoothing 等）</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>