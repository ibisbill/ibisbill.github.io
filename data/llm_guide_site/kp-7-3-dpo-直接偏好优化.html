<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>7.3 DPO：直接偏好优化｜Transformer × LLM 学习站</title>
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 7.3</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="active">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>7.3 DPO：直接偏好优化</h1>
  <p class="muted">先读手册要点，再看公式/代码/交互 demo 与论文/代码库引用。</p>
  <div class="pillrow">
    <span class="pill">模块 7</span>
    <span class="pill"><a href="./module-07-对齐-alignment-rlhf-dpo-rlvr-等.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>手册要点 + 例题答案（原文整理）</h2>
  <p>概述：DPO 用成对偏好直接更新策略，绕开显式 RM 与 PPO，训练更简单且常更稳定。</p>
<p>你需要掌握：</p>
<ul>
<li>数据：chosen 与 rejected 成对样本。</li>
</ul>
<ul>
<li>超参 β：控制偏好强度，相当于 KL 权衡。</li>
</ul>
<ul>
<li>常见变体：IPO、KTO、ORPO 等（不同正则与目标形式）。</li>
</ul>
<p>工程提示：</p>
<ul>
<li>偏好数据质量极关键：错标会强烈写入模型行为。</li>
</ul>
<ul>
<li>对拒答偏好要细分场景，避免把“谨慎”训练成“全拒”。</li>
</ul>
<p>常见误区：</p>
<ul>
<li>误以为 DPO 一定优于 RLHF；两者取决于数据与目标，且可组合。</li>
</ul>
<p>例题与答案：</p>
<div class="qa">
<div class="q">Q1：β 过大可能导致什么现象？</div>
</div>
<div class="qa">
<div class="a">A1：模型过度追随偏好，输出变得极端或多样性下降，并可能牺牲通用能力。</div>
</div>
<div class="qa">
<div class="q">Q2：为什么 DPO 更易落地？</div>
</div>
<div class="qa">
<div class="a">A2：无需单独训练 RM 与在线 PPO 回路，训练管线更短、稳定性通常更好。</div>
</div>
</div>

<div class="section"><h2>公式 / 代码 / 交互补充（重点）</h2>
<h3>DPO：直接偏好优化（Direct Preference Optimization）</h3>
<p>DPO 用“偏好对”直接优化策略模型，避免显式训练 reward model + PPO 的复杂度。一个常见写法是：</p>
<p>$$\mathcal{L}_{\mathrm{DPO}}(\theta)= -\mathbb{E}_{(x,y^+,y^-)}\left[\log \sigma\left(\beta\left(\log \pi_\theta(y^+|x)-\log \pi_\theta(y^-|x) - \log \pi_{\mathrm{ref}}(y^+|x)+\log \pi_{\mathrm{ref}}(y^-|x)\right)\right)\right]$$</p>
<p class="muted">$\pi_{ref}$ 通常是 SFT 模型；括号里是“相对偏好优势”，$\beta$ 控制对齐强度。</p>

<h3>工程实现要点（读代码时关注）</h3>
<ul>
  <li>如何计算 $\log \pi(y|x)$：通常是 token-level logprob 求和（只对回答区间）</li>
  <li>mask/packing：对话模板导致不同样本长度差异，必须正确 mask padding</li>
  <li>稳定性：对 logit 做 clamp、对 $\beta$ 做调参，避免训练早期过强偏好</li>
</ul>

<h3>原论文与典型代码库</h3>
<ul>
  <li>论文：Rafailov et al., 2023, <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization</a></li>
  <li>代码：<a href="https://github.com/huggingface/trl">huggingface/trl</a>（包含 DPOTrainer 等实现）</li>
  <li>对齐背景：Ouyang et al., 2022, <a href="https://arxiv.org/abs/2203.02155">InstructGPT (RLHF)</a></li>
</ul>
</div>

    <div class="pager">
      <a href="kp-7-2-rlhf-三阶段与-ppo-关键点.html"><strong>7.2 RLHF：三阶段与 PPO 关键点</strong></a>
      <a href="kp-7-4-rlvr-可验证奖励的强化学习.html"><strong>7.4 RLVR：可验证奖励的强化学习</strong></a>
    </div>
    <div class="footer">
      <div>来源：`Transformer_LLM_Study_Guide_CN.pdf`（生成日期 2026-01-18）</div>
      <div><a href="../transformers/Transformer_LLM_Study_Guide_CN.pdf">打开原 PDF</a></div>
    </div>
  </div>
</body>
</html>
