<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>7.3 DPO：直接偏好优化｜Transformer × LLM 学习站</title>
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 7.3</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="active">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>7.3 DPO：直接偏好优化</h1>
  <p class="muted">探讨如何绕过复杂的奖励模型训练与 PPO 强化学习，直接利用偏好数据对模型进行对齐。</p>
  <div class="pillrow">
    <span class="pill">模块 7</span>
    <span class="pill"><a href="./module-07-对齐-alignment-rlhf-dpo-rlvr-等.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. DPO (Direct Preference Optimization) 背景</h3>
  <p>传统的 RLHF (PPO) 流程非常复杂，需要维护四个模型（Policy, Reference, Reward, Value），且训练稳定性极差。DPO 的核心贡献是：证明了可以从偏好数据直接推导出最优策略，从而将强化学习问题转化为一个简单的<strong>监督分类问题</strong>。</p>

  <h3>2. DPO 损失函数</h3>
  <p>DPO 通过比较模型在“好的回答” ($y_w$) 和“坏的回答” ($y_l$) 上的 log 概率差异，并扣除参考模型（SFT 模型）的偏置，来直接更新权重：</p>
  <p>$$\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]$$</p>
  <ul>
    <li>$\sigma$ 是 Sigmoid 函数。</li>
    <li>$\beta$ 是超参数（通常在 0.1 到 0.5 之间），控制了对齐的强度。$\beta$ 越大，模型越倾向于追随偏好数据，但泛化能力可能受损。</li>
    <li>$\pi_{\text{ref}}$ 是原始的 SFT 模型，保持冻结。</li>
  </ul>

  <h3>3. DPO 的优势</h3>
  <ul>
    <li><strong>简单高效:</strong> 无需训练奖励模型，无需复杂的强化学习超参调优。</li>
    <li><strong>稳定性高:</strong> 本质上是带权重的交叉熵训练，不容易像 PPO 那样由于采样不稳定而崩溃。</li>
    <li><strong>显存占用低:</strong> 训练时只需要 Policy 模型和 Reference 模型的输出，资源消耗远低于 PPO。</li>
  </ul>
</div>

<div class="section">
  <h2>工程要点总结</h2>
  <ul>
    <li><strong>数据质量是生命线:</strong> 相比 RLHF，DPO 对数据标注错误极其敏感。如果一对 $y_w$ 和 $y_l$ 被标反了，模型会迅速学到错误的偏好。</li>
    <li><strong>SFT 模型质量:</strong> DPO 的效果高度依赖于 $\pi_{\text{ref}}$。如果 SFT 模型本身太弱，DPO 也很难将其对齐到高水平。</li>
    <li><strong>变体衍生:</strong> 目前业界衍生出了 IPO (Identity Preference Optimization)、KTO (Kahneman-Tversky Optimization)、ORPO (Odds Ratio Preference Optimization) 等变体，试图解决 DPO 在某些场景下过拟合或对偏好对依赖过强的问题。</li>
  </ul>

  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：$\beta$ 过大可能导致什么现象？</div>
    <div class="a">A1：$\beta$ 相当于正则化强度的倒数。过大的 $\beta$ 会迫使模型极度迎合标注数据中的偏好，导致模型在生成时过于刻意或死板（多样性下降），甚至会因为过度拟合某些特定的偏好模式而损害其通用的推理和知识水平。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：为什么 DPO 更易落地？</div>
    <div class="a">A2：它将复杂的、带有环境随机性的强化学习任务，转化为了确定性的、基于静态数据集的监督学习任务。开发者无需处理 PPO 中的优势函数估算、Value 网络热启动、采样效率等高门槛问题，使用现有的监督微调工具链即可快速实现对齐。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-7-2-rlhf-三阶段与-ppo-关键点.html"><strong>7.2 RLHF：三阶段与 PPO 关键点</strong></a>
      <a href="kp-7-4-rlvr-可验证奖励的强化学习.html"><strong>7.4 RLVR：可验证奖励的强化学习</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>