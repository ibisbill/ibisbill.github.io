<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>5.5 内存与算子优化（checkpoint/FlashAttention/fused）｜Transformer × LLM 学习站</title>
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 5.5</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>5.5 内存与算子优化</h1>
  <p class="muted">探讨在硬件显存受限的情况下，如何通过激活重计算、FlashAttention 和算子融合技术训练更大的模型。</p>
  <div class="pillrow">
    <span class="pill">模块 5</span>
    <span class="pill"><a href="./module-05-预训练工程与技巧.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 激活重计算 (Activation Checkpointing / Gradient Checkpointing)</h3>
  <p>在神经网络中，反向传播需要前向传播时的激活值。为了节省显存，我们不保存所有层的激活值，而是只保存部分关键位置（Checkpoint）的激活值。</p>
  <ul>
    <li><strong>操作:</strong> 当需要被丢弃的激活值进行反向传播计算时，重新运行一次该层的前向计算。</li>
    <li><strong>权衡:</strong> 极大降低了显存占用（从 $O(L)$ 降低到 $O(\sqrt{L})$），但增加了约 33% 的计算开销（多跑了一次 Forward）。</li>
  </ul>

  <h3>2. FlashAttention</h3>
  <p>FlashAttention 是一种具有 IO 感知能力的精确注意力算法。它通过分块计算（Tiling）和重计算（Recomputation），在 GPU 的高速 SRAM 中完成计算，避免了频繁读写全局显存（VRAM）。</p>
  <ul>
    <li><strong>显存优化:</strong> 将注意力矩阵的显存占用从 $O(S^2)$ 降低到 $O(S)$。</li>
    <li><strong>速度提升:</strong> 相比标准注意力，训练速度可提升 2-4 倍，尤其在长序列下优势巨大。</li>
  </ul>

  <h3>3. 算子融合 (Kernel Fusion / Fused Kernels)</h3>
  <p>在深度学习中，连续的简单算子（如 <code>Add + LayerNorm + Dropout</code>）如果单独运行，会产生多次显存读写。<strong>算子融合</strong>将这些操作合并为一个 GPU Kernel。</p>
  <ul>
    <li><strong>优点:</strong> 显著减少了中间变量在显存中的存取（Memory Bandwidth 瓶颈），并降低了 Kernel 启动的开销。</li>
    <li><strong>实现:</strong> 常用工具包括 OpenAI Triton、DeepSpeed Fused Kernels。</li>
  </ul>
</div>

<div class="section">
  <h2>工程要点总结</h2>
  <ul>
    <li><strong>显存占用分析 (Profiling):</strong> 显存占用主要由：模型参数、优化器状态、梯度、激活值（Activation）四部分组成。优化激活值通常是长序列训练的关键。</li>
    <li><strong>混合精度配合:</strong> 算子优化通常与 FP16/BF16 混合精度配合使用，以最大化 Tensor Core 的吞吐。</li>
    <li><strong>重计算粒度:</strong> 可以选择全量重计算（Full Checkpointing）或部分重计算（Selective Checkpointing），平衡显存节省与计算代价。</li>
  </ul>

  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：Activation checkpointing 的代价是什么？</div>
    <div class="a">A1：它的主要代价是增加了计算时间。为了节省显存，模型在反向传播时必须重新计算那些没有被保存的激活值，这意味着对于受保护的部分，模型实际上运行了两次前向传播。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：为什么 fused kernel 会更快？</div>
    <div class="a">A2：在现代 GPU 上，计算通常远快于内存读写（即访存受限 Memory Bound）。Fused kernel 通过在一次访存过程中完成多个操作，极大地减少了数据在 GPU 核心与显存之间来回搬运的次数，从而突破了显存带宽的瓶颈。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-5-4-分布式训练并行-dp-tp-pp-zero-fsdp.html"><strong>5.4 分布式训练并行（DP/TP/PP/ZeRO/FSDP）</strong></a>
      <a href="kp-5-6-训练监控-调试与故障处理.html"><strong>5.6 训练监控、调试与故障处理</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>