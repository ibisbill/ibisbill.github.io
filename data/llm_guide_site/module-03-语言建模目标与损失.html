<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>3 语言建模目标与损失｜Transformer × LLM 学习站</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/vs2015.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">模块 3</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>3 语言建模目标与损失</h1>
  <p class="muted">本模块涵盖了大语言模型训练中最核心的目标函数、损失计算以及保证训练稳定性的正则化技巧。</p>
</div>

<div class="grid">
  
<div class="card third">
  <h2><a href="./kp-3-1-自回归语言建模-causal-lm.html">3.1 自回归语言建模（Causal LM）</a></h2>
  <p class="muted">大模型的主流预训练方式，基于上文预测下一个 token。</p>
</div>

<div class="card third">
  <h2><a href="./kp-3-2-masked-lm-denoising-目标.html">3.2 Masked LM / Denoising 目标</a></h2>
  <p class="muted">双向上下文建模，常见于 Encoder-only 与去噪预训练模型。</p>
</div>

<div class="card third">
  <h2><a href="./kp-3-3-seq2seq-条件生成损失.html">3.3 Seq2Seq 条件生成损失</a></h2>
  <p class="muted">给定输入序列生成输出序列的条件概率建模。</p>
</div>

<div class="card third">
  <h2><a href="./kp-3-4-稳定性与正则化技巧-z-loss-label-smoothing-等.html">3.4 稳定性与正则化技巧</a></h2>
  <p class="muted">标签平滑、Z-Loss 等保证大规模训练不发散的技巧。</p>
</div>

</div>

<div class="section">
  <h2>模块概述</h2>
  <p>理解语言建模目标是深入掌握 LLM 的基础。自回归目标决定了 GPT 系列的生成特性，而掩码目标则支撑了 BERT 系列的理解能力。在工程实践中，如何通过设计合理的损失函数与正则化项来保证训练过程的稳定性（如防止 Logits 溢出）至关重要。</p>
</div>

    <div class="pager">
      <a href="kp-2-6-参数效率与结构演进-kv-gqa-moe-flashattention.html"><strong>2.6 参数效率与结构演进（KV/GQA/MoE/FlashAttention）</strong></a>
      <a href="kp-3-1-自回归语言建模-causal-lm.html"><strong>3.1 自回归语言建模（Causal LM）</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>