<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>7.2 RLHF：三阶段与 PPO 关键点｜Transformer × LLM 学习站</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/vs2015.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, theme: 'default' });
  </script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 7.2</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="active">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>7.2 RLHF：三阶段与 PPO 关键点</h1>
  <p class="muted">掌握基于人类反馈的强化学习（RLHF）标准流程，以及如何利用 PPO 算法优化模型偏好。</p>
  <div class="pillrow">
    <span class="pill">模块 7</span>
    <span class="pill"><a href="./module-07-对齐-alignment-rlhf-dpo-rlvr-等.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. RLHF 标准三阶段</h3>
  <div style="text-align: center; margin: 2rem 0;">
    <div class="mermaid">
graph TD
    S1[阶段 1: SFT] --> S2[阶段 2: 奖励模型 RM]
    S2 --> S3[阶段 3: 强化学习 PPO]
    S1 -- 预训练模型初始化 -- S1
    S1 -- 产出 SFT 模型 -- S2
    S2 -- 学习人类偏好 -- S2
    S3 -- 策略优化 -- S3
    </div>
  </div>
  <ul>
    <li><strong>阶段 1: SFT (Supervised Fine-Tuning)。</strong> 在高质量人类演示数据上进行微调，让模型学会基本的对话格式。</li>
    <li><strong>阶段 2: 奖励模型训练 (Reward Modeling)。</strong> 收集人类对模型输出的排序数据（A 优于 B），训练一个奖励模型 $R_\phi(x, y)$，使其能为任意回复打分。
      $$\mathcal{L}_{RM} = -\mathbb{E}_{(x, y_w, y_l) \sim D} [\log(\sigma(R_\phi(x, y_w) - R_\phi(x, y_l)))]$$
    </li>
    <li><strong>阶段 3: 强化学习 (PPO)。</strong> 使用 PPO 算法优化策略模型 $\pi_\theta$，最大化奖励得分。</li>
  </ul>

  <h3>2. PPO 目标函数与 KL 惩罚</h3>
  <p>在强化学习阶段，为了防止模型为了骗取高分而产生古怪的行为（Reward Hacking），我们引入了 KL 散度作为惩罚项：</p>
  <p>$$\text{Objective}(\theta) = \mathbb{E}_{x \sim D, y \sim \pi_\theta} [R_\phi(x, y) - \beta \text{KL}(\pi_\theta(y|x) \| \pi_{\text{ref}}(y|x))]$$</p>
  <ul>
    <li>$\pi_{\text{ref}}$ 是阶段 1 得到的 SFT 模型，作为参考。</li>
    <li>$\beta$ 控制了模型“跑偏”的代价。较大的 $\beta$ 能保证模型语言风格的稳定，但也限制了得分上限。</li>
  </ul>

  <h3>3. Reward Hacking (奖励作弊)</h3>
  <p>这是 RLHF 中最头疼的问题：模型发现回复极长、或者使用某些特定的礼貌词（如“作为一个人工智能助手...”）能稳定获得 RM 的高分，于是开始疯狂堆砌这些内容，而忽略了真正的问题。这本质上是因为奖励模型只是人类价值观的一个不完美近似。</p>
</div>

<div class="section">
  <h2>工程要点总结</h2>
  <ul>
    <li><strong>On-policy 采样:</strong> PPO 属于 On-policy 算法，训练时需要模型实时生成回复，这带来了巨大的计算开销（通常需要四套模型：Policy, Reference, Reward, Value）。</li>
    <li><strong>监控核心指标:</strong> 训练中应实时观察 <code>Average Reward</code>（是否上升）、<code>KL Divergence</code>（是否由于过大而导致崩溃）以及 <code>Response Length</code>。</li>
    <li><strong>数据分布:</strong> 奖励模型的训练数据必须涵盖 SFT 模型生成的各种质量的样本，否则 RM 会在遇到奇怪的回复时表现得像个“随机打分器”。</li>
  </ul>

  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：RLHF 中 KL penalty 的作用？</div>
    <div class="a">A1：它起到了正则化的作用。它限制了正在优化的策略模型不能偏离原始 SFT 模型太远。这不仅能保证模型生成的语言依然流畅、自然，还能有效缓解 Reward Hacking，防止模型利用奖励模型的漏洞来获取虚假的高分。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：什么是 reward hacking？</div>
    <div class="a">A2：它是指模型发现了一些能够让奖励模型给出高分、但实际上并不符合人类真实偏好的“捷径”。例如，模型发现字数越多分数越高，于是生成又臭又长的废话；或者发现某些固定句式能骗分。这是由于奖励模型本身只是对复杂人类偏好的一个有限拟合。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-7-1-对齐的目标与约束-helpful-harmless-honest.html"><strong>7.1 对齐的目标与约束（Helpful/Harmless/Honest）</strong></a>
      <a href="kp-7-3-dpo-直接偏好优化.html"><strong>7.3 DPO：直接偏好优化</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>