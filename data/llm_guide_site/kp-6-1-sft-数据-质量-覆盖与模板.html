<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>6.1 SFT 数据：质量、覆盖与模板｜Transformer × LLM 学习站</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/vs2015.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 6.1</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>6.1 SFT 数据：质量、覆盖与模板</h1>
  <p class="muted">探讨如何通过高质量的指令数据，将预训练模型转化为能够遵循指令、进行流畅对话的助手。</p>
  <div class="pillrow">
    <span class="pill">模块 6</span>
    <span class="pill"><a href="./module-06-指令微调-sft-与后训练-post-training.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 指令微调 (Supervised Fine-Tuning, SFT)</h3>
  <p>预训练模型本质上是一个“续写器”。SFT 的目标是通过展示大量的“指令-回答”对，让模型学会理解人类意图并按照特定的对话规范行事。与预训练不同，SFT 极其强调<strong>数据质量</strong>而非规模。</p>

  <h3>2. 数据形态与覆盖</h3>
  <ul>
    <li><strong>单轮指令:</strong> “请写一首关于夏天的诗。”</li>
    <li><strong>多轮对话:</strong> 包含上下文依赖的交互。</li>
    <li><strong>任务覆盖:</strong> 包含创作、改写、摘要、代码编写、逻辑推理、数学计算、角色扮演等。</li>
    <li><strong>边界案例:</strong> 包含模型应该拒绝回答的问题（如违法指令），以训练模型的安全性。</li>
  </ul>

  <h3>3. 对话模板 (Chat Templates)</h3>
  <p>为了让模型区分不同的发言角色，必须使用一致的模板。常见的如 ChatML：</p>
  <pre><code>&lt;|im_start|&gt;system
你是一个人工智能助手。&lt;|im_end|&gt;
&lt;|im_start|&gt;user
你好。&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
你好！有什么我可以帮你的吗？&lt;|im_end|&gt;</code></pre>
  <p><strong>注意:</strong> 模板中的特殊 token（如 <code>&lt;|im_start|&gt;</code>）必须被添加到词表中，且在训练和推理时必须保持绝对一致。</p>

  <h3>4. 损失掩码 (Loss Masking)</h3>
  <p>在 SFT 训练中，我们只对 <code>assistant</code> 的输出部分计算交叉熵损失。对于 <code>system</code> 指令、<code>user</code> 提问以及所有模板标签（如 <code>&lt;|im_start|&gt;</code>），其标签位应设为 <code>-100</code> 从而在 Loss 计算中被忽略。</p>
</div>

<div class="section">
  <h2>工程要点总结</h2>
  <ul>
    <li><strong>少而精:</strong> 1,000 条极高质量的专家编写数据，效果往往好于 100,000 条低质量、有噪声的合成数据。</li>
    <li><strong>数据审查:</strong> 必须进行人工或强模型（如 GPT-4）的质量打分，剔除逻辑不通、格式错误或含糊其辞的样本。</li>
    <li><strong>多样性:</strong> 确保指令的动词（Verb）和任务域（Domain）足够多样，避免模型陷入特定回复套路（模式崩塌）。</li>
  </ul>

  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：多轮对话数据为什么需要严格模板？</div>
    <div class="a">A1：模型在 SFT 阶段不仅学习内容，还在学习对话的行为模式。严格的模板能帮助模型清晰地界定上文来源、当前轮次以及角色边界。模板不一致（如多了个换行符）会干扰位置编码和语义理解，导致模型产生角色混淆（如开始复读用户输入）。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：SFT 中常用的 loss mask 策略？</div>
    <div class="a">A2：标准做法是只对 assistant 角色生成的回复部分计算 Loss。这是因为我们的训练目标是“给定背景和问题，生成好的回答”。如果不掩盖用户输入，模型会分出算力去学习如何预测用户的提问，这不仅没有必要，还可能导致模型产生“复读机”倾向。</div>
  </div>
</div>

    <div class="pager">
      <a href="module-06-指令微调-sft-与后训练-post-training.html"><strong>6 指令微调（SFT）与后训练（Post-training）</strong></a>
      <a href="kp-6-2-sft-训练策略-全参-vs-peft-lora-qlora.html"><strong>6.2 SFT 训练策略：全参 vs PEFT（LoRA/QLoRA）</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>