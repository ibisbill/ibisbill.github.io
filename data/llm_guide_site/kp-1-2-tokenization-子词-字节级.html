<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>1.2 Tokenization（子词/字节级）｜Transformer × LLM 学习站</title>
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, theme: 'default' });
  </script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 1.2</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>1.2 Tokenization（子词/字节级）</h1>
  <p class="muted">先读手册要点，再看公式/代码/交互 demo 与论文/代码库引用。</p>
  <div class="pillrow">
    <span class="pill">模块 1</span>
    <span class="pill"><a href="./module-01-nlp-llm-表示与-tokenization.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 为什么需要子词 (Subword)？</h3>
  <p>传统的 Word-level 模型面临 OOV (Out-of-Vocabulary) 问题，而 Character-level 模型序列过长且语义缺失。子词平衡了两者的优缺点。</p>

  <h3>2. 主流分词算法</h3>
  <ul>
    <li><strong>BPE (Byte Pair Encoding):</strong> 
      从字符开始，不断合并出现频率最高的相邻字符对。
      GPT 系列使用。
    </li>
    <li><strong>WordPiece:</strong> 
      与 BPE 类似，但合并的标准不是频率，而是似然增益（Likelihood gain）。
      BERT 系列使用。
    </li>
    <li><strong>Unigram LM:</strong> 
      从一个巨大的词表开始，不断删除对整体似然影响最小的词。
      T5、SentencePiece 常用。
    </li>
  </ul>

  <h3>3. BBPE (Byte-level BPE)</h3>
  <p>Llama 等模型不再使用字符作为基本单位，而是使用 <strong>Byte</strong>。这样可以将任何 UTF-8 文本编码为 256 个基础 byte 的组合，彻底解决 OOV 问题，且对多语言极其友好。</p>

  <h3>代码示例：BPE 核心逻辑模拟</h3>
  <pre><code class="language-python">import re, collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

# 初始词表：词 -> 频率
vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}
for i in range(10): # 迭代合并 10 次
    pairs = get_stats(vocab)
    if not pairs: break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(f"Iter {i}: Best pair {best}")
</code></pre>

  <h3>算法对比示意图</h3>
  <div style="text-align: center;">
    <div class="mermaid">
graph TD
    A[分词算法] --> B[BPE]
    A --> C[WordPiece]
    A --> D[Unigram]
    B --> B1[基于频率合并]
    C --> C1[基于似然增益合并]
    D --> D1[基于概率逐步修剪]
    </div>
  </div>
</div>

<div class="section">
  <h2>手册要点 + 例题答案</h2>
  <p>概述：Tokenization 把文本映射为离散符号序列，决定了训练目标的粒度、序列长度与跨语言鲁棒性。</p>
  <p>工程提示：</p>
  <ul>
    <li>不要在训练和推理时使用不同的 tokenizer；甚至版本微调（如加了几个 token）都可能导致权重失效。</li>
    <li>不同分词器对中文/代码的压缩率差异巨大，影响上下文窗口的实际长度。</li>
  </ul>
  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：BPE 的核心迭代步骤是什么？</div>
    <div class="a">A1：统计相邻符号对的频次，反复合并最频繁的对，直到达到词表大小或停止条件。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：为什么 byte-level tokenization 更鲁棒？</div>
    <div class="a">A2：任何文本都可表示为字节序列，避免 OOV；对噪声、emoji、混合语言更稳定。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-1-1-文本规范化与预处理.html"><strong>1.1 文本规范化与预处理</strong></a>
      <a href="kp-1-3-表示学习-embedding-与位置编码.html"><strong>1.3 表示学习、Embedding 与位置编码</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>
