<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>0.2 深度学习基础｜Transformer × LLM 学习站</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/vs2015.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, theme: 'default' });
  </script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 0.2</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>0.2 深度学习基础</h1>
  <p class="muted">先读手册要点，再看公式/代码/交互 demo 与论文/代码库引用。</p>
  <div class="pillrow">
    <span class="pill">模块 0</span>
    <span class="pill"><a href="./module-00-预备基础.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 反向传播与计算图 (Backpropagation)</h3>
  <p>深度学习的本质是通过梯度下降优化损失函数。反向传播利用链式法则（Chain Rule）高效计算梯度。</p>
  <ul>
    <li><strong>链式法则:</strong> 若 $z = f(y)$ 且 $y = g(x)$，则 $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}$。</li>
    <li>在 Transformer 中，梯度从最终的交叉熵损失出发，穿过每一个线性层、Attention 层和归一化层，更新权重。</li>
  </ul>

  <h3>2. 归一化 (Normalization)</h3>
  <p>归一化有助于缓解深层网络的梯度消失或爆炸问题，加速收敛。</p>
  <ul>
    <li><strong>LayerNorm (LN):</strong> 对单个样本的所有特征进行归一化。
      $$\text{LN}(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$
      其中 $\mu$ 和 $\sigma^2$ 是均值和方差。
    </li>
    <li><strong>RMSNorm (Root Mean Square Norm):</strong> Llama 等模型常用的变体，省去了均值计算，效率更高。
      $$\text{RMSNorm}(x) = \gamma \frac{x}{\sqrt{\text{Mean}(x^2) + \epsilon}}$$
    </li>
    <li><strong>Pre-LN vs Post-LN:</strong> 
      <ul>
        <li><strong>Post-LN:</strong> $\text{LayerNorm}(x + \text{SubLayer}(x))$。早期 Transformer 使用，训练较难。</li>
        <li><strong>Pre-LN:</strong> $x + \text{SubLayer}(\text{LayerNorm}(x))$。目前主流，模型更稳定，支持更深层。</li>
      </ul>
    </li>
  </ul>

  <h3>3. 正则化 (Regularization)</h3>
  <ul>
    <li><strong>Dropout:</strong> 训练时随机丢弃神经元，防止过拟合。在超大规模 LLM 中，有时会降低 dropout 甚至关闭。</li>
    <li><strong>Weight Decay:</strong> 在损失函数中添加 $\lambda \Vert \theta \Vert^2$。在 AdamW 中，它是直接作用于权重更新而非梯度。</li>
    <li><strong>Label Smoothing:</strong> 将硬标签 [0, 1] 转换为软标签 $[\epsilon/K, 1-\epsilon]$，防止模型过于自信。</li>
  </ul>

  <h3>代码示例：RMSNorm 实现</h3>
  <pre><code class="language-python">import torch
import torch.nn as nn

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        # x: (B, S, D)
        # 计算均方根
        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        # 归一化并放缩
        return x * rms * self.weight

# 测试
x = torch.randn(2, 4, 128)
norm = RMSNorm(128)
output = norm(x)
print(f"Output shape: {output.shape}")
</code></pre>

  <h3>训练动态示意图</h3>
  <div style="text-align: center;">
    <div class="mermaid">
graph LR
    A[输入] --> B[LayerNorm]
    B --> C[Attention/MLP]
    C --> D[Residual Add]
    D --> E[下一层]
    subgraph Pre-LN 结构
    B
    C
    D
    end
    </div>
  </div>
</div>

<div class="section">
  <h2>手册要点 + 例题答案</h2>
  <p>概述：LLM 训练本质上是大规模序列建模的梯度优化。需要理解反向传播、归一化、正则化对稳定性的影响。</p>
  <p>工程提示：</p>
  <ul>
    <li>大模型常见：dropout 变小或关闭；weight decay 依数据与规模调参。</li>
    <li>训练不稳定优先检查：学习率、warmup、混合精度与溢出。</li>
  </ul>
  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：为什么 Pre-LN 往往比 Post-LN 更稳？</div>
    <div class="a">A1：Pre-LN 让残差支路的梯度更直接传播，缓解深层梯度消失/爆炸，训练更稳定。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：Weight Decay 与 L2 正则完全等价吗？</div>
    <div class="a">A2：对 AdamW 而言不等价：AdamW 将权重衰减与梯度更新解耦，更符合经典 weight decay 的含义。</div>
  </div>
</div>

    <div class="pager">
      <a href="kp-0-1-数学基础.html"><strong>0.1 数学基础</strong></a>
      <a href="kp-0-3-工程与系统基础.html"><strong>0.3 工程与系统基础</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>
