<!doctype html>
<html lang="zh-Hans">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>1.3 表示学习（Embedding 与位置编码）｜Transformer × LLM 学习站</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/site.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/vs2015.min.css" />
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
      },
      options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true, theme: 'default' });
  </script>
  <script defer src="./assets/site.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      if (window.hljs) window.hljs.highlightAll();
    });
  </script>
</head>
<body>
  <div class="topbar">
    <div class="container topbar-inner">
      <div class="brand">
        <div class="brand-badge"></div>
        <div>
          <div>Transformer × LLM 学习站</div>
          <div class="brand-sub">知识点 1.3</div>
        </div>
      </div>
      <div class="navlinks">
        <a href="./index.html" class="">目录</a>
        <a href="./module-02-transformer.html" class="">Transformer</a>
        <a href="./module-07-alignment.html" class="">对齐</a>
        <a href="./module-08-inference.html" class="">推理</a>
      </div>
    </div>
  </div>

  <div class="container">
    
<div class="hero">
  <h1>1.3 表示学习（Embedding 与位置编码）</h1>
  <p class="muted">先读手册要点，再看公式/代码/交互 demo 与论文/代码库引用。</p>
  <div class="pillrow">
    <span class="pill">模块 1</span>
    <span class="pill"><a href="./module-01-nlp-llm-表示与-tokenization.html">返回模块页</a></span>
  </div>
</div>

<div class="section">
  <h2>核心知识点详细讲解</h2>

  <h3>1. 词嵌入 (Token Embedding)</h3>
  <p>将离散的 token ID 映射为高维稠密向量。模型学习到的每个维度都代表某种潜在的语义特征。</p>
  <ul>
    <li><strong>Weight Tying:</strong> 输入 Embedding 层与输出 Softmax 前的线性层共享权重。这可以减少约 1/3 的模型参数量（对于大词表模型），并被证明能提高模型性能。</li>
  </ul>

  <h3>2. 位置编码 (Positional Encoding)</h3>
  <p>由于 Transformer 的 Self-Attention 是置换不变的（无法区分顺序），必须手动加入位置信息。</p>
  <ul>
    <li><strong>绝对位置编码 (Absolute PE):</strong> 为每个位置 $pos$ 分配一个固定的向量。
      <ul>
        <li><strong>正弦位置编码:</strong> 
          $PE(pos, 2i) = \sin(pos / 10000^{2i/d})$
          $PE(pos, 2i+1) = \cos(pos / 10000^{2i/d})$
        </li>
        <li><strong>学习式编码:</strong> 将位置也作为可学习的参数（如 GPT-2）。</li>
      </ul>
    </li>
    <li><strong>旋转位置编码 (RoPE):</strong> 
      通过旋转矩阵将位置信息注入。它具有<strong>相对位置特性</strong>，且在长文本外推上表现优异。
      公式简化形式：$f(x, pos) = x \cdot e^{i \cdot pos \cdot \theta}$。
    </li>
    <li><strong>ALiBi:</strong> 
      在 Attention Score 上直接加一个随距离增加而减小的偏置，无需 Embedding，外推性极强。
    </li>
  </ul>

  <h3>代码示例：RoPE 的 PyTorch 实现片段</h3>
  <pre><code class="language-python">import torch

def apply_rotary_emb(x, cos, sin):
    # x: (batch, seq_len, num_heads, head_dim)
    # 将 head_dim 拆分为两半进行旋转
    x1 = x[..., :x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    rotated_x = torch.cat((-x2, x1), dim=-1)
    return (x * cos) + (rotated_x * sin)

# 提示：cos 和 sin 是预计算好的频率张量
</code></pre>

  <h3>位置方案对比示意图</h3>
  <div style="text-align: center;">
    <div class="mermaid">
graph TD
    A[位置信息方案] --> B[绝对位置]
    A --> C[相对位置]
    B --> B1[Sinusoidal 固定]
    B --> B2[Learned 学习]
    C --> C1[RoPE 旋转]
    C --> C2[ALiBi 偏置]
    C --> C3[Relative Bias]
    </div>
  </div>
</div>

<div class="section">
  <h2>手册要点 + 例题答案</h2>
  <p>概述：LLM 通过嵌入把离散 token 转为连续向量，再叠加位置/段落信息。位置编码决定模型如何理解顺序与距离。</p>
  <p>工程提示：</p>
  <ul>
    <li>RoPE 外推需要配合长序列训练或缩放；只改公式不训往往不够。</li>
    <li>注意 mask：causal/padding/prefix-LM 在训练与推理必须一致。</li>
  </ul>
  <p>例题与答案：</p>
  <div class="qa">
    <div class="q">Q1：什么是 weight tying？常见收益是什么？</div>
    <div class="a">A1：输入 embedding 与输出层权重共享，减少参数并常带来轻微泛化收益。</div>
  </div>
  <div class="qa">
    <div class="q">Q2：causal mask 的作用是什么？</div>
    <div class="a">A2：保证位置 t 的预测只依赖 &lt;t 的 token，防止“偷看未来”。</div>
  </div>
</div>

<div class="section">
  <h3>交互 Demo：Sinusoidal PE 可视化</h3>
  <div class="demo" data-demo="posenc">
    <div class="row">
      <div class="col">
        <label>pos</label>
        <input data-role="pos" value="10" />
      </div>
      <div class="col">
        <label>d_model</label>
        <input data-role="dim" value="64" />
      </div>
    </div>
    <div class="row">
      <button class="btn primary" data-action="run">生成</button>
    </div>
    <canvas width="920" height="220" style="width:100%;margin-top:10px;border-radius:14px;border:1px solid rgba(255,255,255,.10);background:rgba(0,0,0,.18)"></canvas>
    <div class="out" data-role="OUT"></div>
  </div>
</div>

    <div class="pager">
      <a href="kp-1-2-tokenization-子词-字节级.html"><strong>1.2 Tokenization（子词/字节级）</strong></a>
      <a href="module-02-transformer-核心结构.html"><strong>2 Transformer 核心结构</strong></a>
    </div>
    <div class="footer">
      <div>© 2026 Transformer × LLM 学习站</div>
    </div>
  </div>
</body>
</html>
